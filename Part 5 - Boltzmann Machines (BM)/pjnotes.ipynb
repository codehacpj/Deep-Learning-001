{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan of attack\n",
    "1. Boltzmann Machine\n",
    "2. Energy-based Models (EBM)\n",
    "3. Restricted Boltzmann Machine(RBM)\n",
    "4. Contrastive Divergence(DBN)\n",
    "5. Deep Belief Networks(DBN)\n",
    "6. Deep Boltzmann Machines(DBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boltzmann Machines\n",
    "Comparing with other models, which have feeds in one direction for hidden nodes, here boltzmann machines have feeds from and to(`bidirectional`) every other nodes(`hyperconnected`) in the structure:<br>\n",
    "`red` ones are the hidden nodes and `blue` ones are visible nodes.<br>\n",
    "`no output layer`<br>\n",
    "<p>\n",
    "    \n",
    "The visible nodes are connected to each other <br>\n",
    "`The assumption of fixed input fails here.`<br>\n",
    "so to make sense of the connections between visible nodes, we have to understand that BMs generate information. <br>\n",
    "`For a BM all the nodes are same`<br>\n",
    "`And all of them are generating states of the system.`<br>\n",
    "\n",
    "<p>\n",
    "    \n",
    "    \n",
    "#### Nuclear Powerplant example:\n",
    "There are lots of things that we measure and take into consideration, but everything that makes up this system can't be taken into account. (say `moisture of soil or speed of the wind` at that particular moment.)<br>\n",
    "\n",
    "<p>\n",
    "    \n",
    "`BMs` are similar to the `Powerplant` as the there are the `visible nodes` which we can and do measure and then there are `parameters` that we don't take into account which are represented by `hidden nodes`<br>\n",
    "\n",
    "The `BMs` are kind of autonomous for generating the `states` for the system.<br>\n",
    "<p>\n",
    "    \n",
    "So rather than being deterministic model it is stochastic model.<br>\n",
    "<p>\n",
    "    \n",
    "Hence when we feed data into the `BM`, it learns about the possible connections(`weights`) between the parameters on its own to represent our system.<br>\n",
    "<p>\n",
    "    \n",
    "Once the Training is done we can use `BM` to monitor our system.<br>\n",
    "\n",
    "Hence in case of our `Nuclear Power Plant` once our fed data is used to come up with a state for a Nuclear power plant learing through good examples, we can detect the normal behaviour for Nuclear power plant by unsuperviesed learning, so we can detect the abnormality(if any). This can't be done using supervised learning as the `Nuclear meltdown` data would be required for it to train that way(and lots of them) which is not the best way to go.<br>\n",
    "\n",
    "`No output layer` because we are not giving an output but coming up with a model that describes our system.`all connected` to come up with the best possible state for the model where any possible parameter can have any possible othr parameter differ for small change of weights.`No directions` as all the parameters for the boltzmann machines are equal for `BMs`<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boltzmann distribution:\n",
    "`BM` uses this distribution to come up with the different states.<br>\n",
    "The probability of the air molecules being in certain unconditional state is inversly proportional to Energy at a given Temperature. i.e for air molecules for example to have high energy, and fill up the a corner space in a room, probability of this happening is very low. We observe the lowest energy state and high probability of air molecules distribution all over the room.<br>\n",
    "This happens for every other systems, ink in water finds the lowest energy state, oil in water....<br>\n",
    "<p>\n",
    "    \n",
    "  \n",
    "`BMs` are based on this similar concept.<br>\n",
    "`Energy` in `BM` is defined by the weights of synapses. Once the system is trained, weights are set, Then system based on those weights will always try to find `lowest energy state possible` for itself.<br>\n",
    "That is why they are called `Energy-Based Models`<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Restricted Boltzmann Machines(RBM):\n",
    " Full Boltzmann machines are hard to implement, it is really hard to compute full BM(As the number of connections grow exponentially with number of nodes).<br>\n",
    "<p>\n",
    "\n",
    "The `restricted BMs` were proposed, similar but hidden nodes can't connect to each other and visible nodes cannot connect to each othter.<br>\n",
    "<p>\n",
    "    \n",
    "#### Using Restricted BMs to make a movie recommender system:\n",
    "say BM is working on 6 movies.<br>\n",
    "Training is done through process called `contrastive diversions`.<br>\n",
    "The BM is trained with our specific set of movies (fed to it as data) so it will be based on it and behave as movie recommender system intensive to our data rather than being a general movie recommender system.<br>\n",
    "<p>\n",
    "    \n",
    "While training it learns how to `allocate its hidden nodes to certain features`.Say it identifies some features like (`Genre`,`Actors`,`Awards`,`Director`).<br>\n",
    "For example: lots of data are fed to the BM. say it has `movies`---> columns and `users` --->rows.<br>\n",
    "every cell has rating given by user to the movie.<br>\n",
    "(like or dislike 0 or 1 and empty for user if not watched).<br>\n",
    "<br>\n",
    "Now BM is able to understand better our system and it adjusts itself to be a better representation of our system(to reflect all interconnectivity).<br>\n",
    "It comes up with the connections between the likes of people, what movies are liked and are not liked.<br>\n",
    "\n",
    "<br>\n",
    "For every node, it learns from the input and accepts the values in hidden nodes and then tries to `reconstruct` the input based on hidden node. while doing so it assigns the weights to the constructs according to the training that was done initially.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Divergence\n",
    "Since we don't have a directed network we can't use something like backpropagation for BMs to learn. `Contrastive Divergence` is the way by which they learn.<br>\n",
    "<p>\n",
    " \n",
    "once the input nodes are put in the network with randomly assigned weights, at very start the machine calculates the hidden nodes. Then the hidden nodes reconstructs the input using the exact same weights.<br>\n",
    "Even though the weights are same, the reconstructed input is not going to be the same.<br>\n",
    "This happens because the input nodes are not connected to each other.<br>\n",
    "Again the we construct the hidden values and reconstruct the input.... till at some point we get some reconstructed input values which are such that wehen we feed them into RBM and then we try to reconstruct them agiain, we get the same value.This whole process is called `Gibbs Sampling`.<br>\n",
    "Then we say that our model is actually modeling our inputs.<br>\n",
    "<p>\n",
    "    \n",
    "In terms of `curve`:\n",
    "At constant weights:<br>\n",
    "These weights dictate the shape of these energy curves.<br>\n",
    "while in the `Gibbs Sampling` the Energy keeps on deccreasing and the the point we reach at lowest energy state, we get to the end where reconstructed input values and hidden values bring the machine to lowest energy state.<br>\n",
    "<p>\n",
    "    \n",
    "Here because of the random weights we get the random recreated input values and we want the model to do it on our inputs.<br>\n",
    "We have the energy curve, shape of it is governed by the weights in the system, and to get to the minimal energy possible. The goal is to redesign the system so that energy curve reflects our the system ie. when we give our values, the system is alrealdy in lowest energy state possible.<br>\n",
    "so the formula gives us the way to adjust weights so that we reach the minimum energy state.<br>\n",
    "\n",
    "`Hinton's shortcut`:<br>\n",
    "According to Hinton, even if we take the first two passes, and we don't wait till the end where it converges this is sufficient to understand how to adjust your curve. `(CD1(contrastive divergence 1))` gives us the direction of lowering energy and thus we can adjust the weights to make this step minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing stage:\n",
    "Import, prepare the training set and test set, Number of users and number of movies and convert them into an array where we have users in rows and movies in the columns.<br>\n",
    "Lastly we will convert the data into `torch tensors`.<br>\n",
    "<p>\n",
    "    \n",
    "Then we will start the steps specific to boltzmann machines.<br>\n",
    "We will start dealing with binary ratings, then create an architecture of neural network(`Restricted Boltzmann machines` which is a `probabilistic graphical model`).<br>\n",
    "<p>\n",
    "    \n",
    "### Data preprocessing phase:\n",
    "##### We will use the dataset from grouplens.org/datasets/movielens with 100k ratings and 1m ratings to predict if the new user will like a movie or not and how much rating would he/she give to this movie.\n",
    "\n",
    "<p>\n",
    "    \n",
    "`Gibbs Sampling` discussed above is based on `Markov chain Monte Carlo Techniques`, the theory is in the pdf included in the dataset folder.<br>\n",
    "<p>\n",
    "    \n",
    "We will use `k-step contrastive divergence` algorithm to implement our `RBM`.<br>\n",
    "\n",
    "<p>\n",
    "    \n",
    "#### Importing dataset\n",
    "1. Import the libraries : numpy,pandas,torch, `torch.nn`(The module of Torch to implement neural networks). `torch.nn.parallel` for `parallel computations`, `torch.optim` for `optimizer`,`torch.utils.data` is a tool that we will use, `Variable from torch.autograd` for stochastic gradient.<br>\n",
    "<p>\n",
    "    \n",
    "2. Import the whole dataset `ml-1m/movies.dat`, separator is `::`, there is no column names, so `header` will be specified as `None` to denote that the file doesn't have column names.<br> we will specify `engine`=`python` to make sure that the dataset is imported correctly.<br> last argument is `encoding` set to `latin-1` to encorporate the special characters included in the movie title.<br> Has `movieid,moviename,genre`<br>\n",
    "<p>\n",
    "    \n",
    "3. import the users `ml-1m/users.dat` with same arguments.<br> has `information on users: userid,gender,age,code for user's job,visit code`<br>\n",
    "\n",
    "<p>\n",
    "    \n",
    "4. import the ratings `ml-1m/ratings.dat` with same arguments.<br> has `information on ratings: corresponds to user(1 means first user), movie ids,ratings,timestamps for the ratings`<br>\n",
    "\n",
    "<p>\n",
    "    \n",
    "#### prepare train set and test set:\n",
    "\n",
    "5. `ml-100k/` has u1.base,u1.test... all are different trianing set and test set. All these are for the k-fold cross validations. <br> Here we will use `u1.base` and `u1.train`.<br> import the training set and test sets. <br> `delimiter='\\t'` paramenter<br> This will be similar to the ratings dataframe above.<br><p>\n",
    "\n",
    "6. Convert the training set into array so that it will further be used to prepare pytorch tensors.<br> Use `np.array` for this.<br> will take argument `dtype='int'` to convert this array into array of integers as it has ids ratings and time stamps.<br> \n",
    "<p>\n",
    "    \n",
    "7. Will do the same for test set.<br><p>\n",
    "\n",
    "#### Getting number of users and movies (Converting the above data to create matrix with rows as users and columns as movies) for both training set and test set. We do this to make data \n",
    "<br>\n",
    "if the train set doesn't have a data on some users for certain movies---> we will use `0` for ratings.\n",
    "\n",
    "8. To get the number of users---> take `max(max(training_set[:,0]),max(test_set[:,0]))` same to get the number of movies with index 1\n",
    "\n",
    "<br><p>\n",
    "\n",
    "9. To convert the data into required matrix, we write a function.<br> For the matrix we will create list of list.<br> for every user there will be a list of ratings corresponding to a movie.<br>\n",
    "\n",
    "<p>\n",
    " \n",
    "#### Converting the data into torch tensors \n",
    "`Tensors` are arrays that contain elements of a single data type, it is a multidimentional matrix, which is a pytorch array.<br>\n",
    "\n",
    "10. torch.FloatTensor() will do the job for both training and test list of lists.<br><p>\n",
    "\n",
    "#### converting the users ratings to like or didn't like(binary responses).\n",
    " \n",
    "11. for the ones a user didn't rate we replace the 0's with -1s.\n",
    "<p>\n",
    "    \n",
    "    \n",
    "#### Architecture of Neural Network.\n",
    "done by creating a class for restricted bolzmann machines.<br> the class will have 3 methods----> one to initialize rbm object, another one to `sample_h` ----> it will sample the probabilities of the hidden nodes given the visible nodes, and the last function `sample_v` that will sample the probabilities of the visible nodes given the hidden nodes.<br><p>\n",
    "    \n",
    "12. Initialize the class<br> passing `number of hidden nodes and number of visible nodes` while initializing the class lets initialize the `weights`, `bias for hidden nodes` and `bias for visible nodes` all of them will be random.<br><p>\n",
    "\n",
    "13. Function 2 for sampling the hidden nodes, according to the probabilities `ph` given `v`, it is `sigmoid function` we need this because during the training to approximate the log likelihood gradient through Gibbs sampling and to apply Gibbs sampling, we need to compute the probabilities of the hidden nodes.<br> `sample_h` say we have 100 hidden nodes, this function will sample the activations of the nodes, for each of 100 hidden nodes, it will activate them according to a certain probability that we will compute in the same function. for each of these hidden nodes, this probability is `ph`(probability of hidden nodes having value `1` according to the given value of `v`) given `v`. `x` is the argument which will correspond to visible neuron `v`. <br> <p> \n",
    "    \n",
    "According to the formula given, we will calculate `wx` using `torch.mm()` with `x` and `weights` and `activation` will be equal to `wx` plus the `hidden bias`.<br>\n",
    "We return both the probabilities and even the sampling using Bernauli sampling for the probabilities.<br>\n",
    "<p>\n",
    "    \n",
    "14. Same is done for `pv` given `h`.<br><p>\n",
    "    \n",
    "#### The last function will be about the contrastive divergence that we will use the approximate likelihood gradient.\n",
    "\n",
    "15. `Train` function according to the paper to update the weights and biases.<br><p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
