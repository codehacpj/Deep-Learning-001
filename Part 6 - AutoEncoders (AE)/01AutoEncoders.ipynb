{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "directed neural network<br>\n",
    "It encodes itself.![test image](Screenshot from 2018-08-19 12-00-23.png)<p>\n",
    "\n",
    "i/p   ---> hidden layers ----> o/p similar to the i/ps.![Screenshot%20from%202018-08-23%2018-14-04.png](Screenshot%20from%202018-08-23%2018-14-04.png)\n",
    "\n",
    "![Screenshot%20from%202018-08-23%2018-20-21.png](Screenshot%20from%202018-08-23%2018-20-21.png)\n",
    "\n",
    "![Screenshot%20from%202018-08-23%2018-21-19.png](Screenshot%20from%202018-08-23%2018-21-19.png)\n",
    "![Screenshot%20from%202018-08-23%2018-25-13.png](Screenshot%20from%202018-08-23%2018-25-13.png)\n",
    "`self supervised`<br>\n",
    "can be used for feature detection, to build recommender systems, to encode data<br>\n",
    "<p>\n",
    "    \n",
    "\n",
    "How auto encoders work for the recommender systerm:<br>\n",
    "1. we take the ratings given by the users one by one<br>\n",
    "2. for every user when passed to hidden nodes the hidden nodes take weights for most important features, and then pass to output nodes. <br>\n",
    "3. the outputs are compared to the inputs and the errors are calculated, the errors are back propagated to fix the weights for the next iteration. <br>\n",
    "4. Repeated for all users for certain epochs.\n",
    "\n",
    "![Screenshot%20from%202018-08-23%2018-27-28.png](Screenshot%20from%202018-08-23%2018-27-28.png)![Screenshot%20from%202018-08-23%2018-28-48.png](Screenshot%20from%202018-08-23%2018-28-48.png)![Screenshot%20from%202018-08-23%2018-28-44.png](Screenshot%20from%202018-08-23%2018-28-44.png)![Screenshot%20from%202018-08-23%2018-29-26.png](Screenshot%20from%202018-08-23%2018-29-26.png)\n",
    "\n",
    "    \n",
    "The Autoencoders can cheat if the number of hidden nodes are equal or more than that of the input nodes(`sometimes when we want to extract more features`). This situation can be dealt with using different kinds of autoencoders.<br>\n",
    "<p>\n",
    "    \n",
    "1. Sparse Autoencoders: <br> similar to the picture above with `regularization technique(which prevents overfitting) which introduces sparsity` is applied in addition. i.e. it introduces a constraint on the loss function or a penalty which doesn't allow the autoencoder to use all of its hidden layer at a time.\n",
    "![Screenshot%20from%202018-08-23%2018-42-56.png](Screenshot%20from%202018-08-23%2018-42-56.png)![Screenshot%20from%202018-08-23%2018-48-58.png](Screenshot%20from%202018-08-23%2018-48-58.png)\n",
    "![Screenshot%20from%202018-08-23%2018-49-35.png](Screenshot%20from%202018-08-23%2018-49-35.png)![Screenshot%20from%202018-08-23%2018-49-17.png](Screenshot%20from%202018-08-23%2018-49-17.png)\n",
    "<p>\n",
    "\n",
    "2. Denoising Autoencoders:<br> Modified version of input values by making random number of inputs `0's` mostly the half of your inputs but again the output values are compared to original inputs.\n",
    "![Screenshot%20from%202018-08-23%2018-51-45.png](Screenshot%20from%202018-08-23%2018-51-45.png)![Screenshot%20from%202018-08-23%2018-53-22.png](Screenshot%20from%202018-08-23%2018-53-22.png)\n",
    "\n",
    "<p>\n",
    "    \n",
    "3. Contractive Autoencoders: <br> It leverages the whole autoencoding, while sending back the error, it introduces the penalty.`workings of it is complex`\n",
    "![Screenshot%20from%202018-08-23%2018-55-46.png](Screenshot%20from%202018-08-23%2018-55-46.png)\n",
    "<p>\n",
    "\n",
    "\n",
    "4. Stacked Autoencoders: <br> Adding another layer of hidden layers---> 2 statges of encoding and one stage of decoding.\n",
    "![Screenshot%20from%202018-08-23%2019-03-51.png](Screenshot%20from%202018-08-23%2019-03-51.png)![Screenshot%20from%202018-08-23%2019-04-27.png](Screenshot%20from%202018-08-23%2019-04-27.png)\n",
    "\n",
    "<p>\n",
    "    \n",
    "5. Deep Autoencoders: <br> stacked restricted boltzmann machines<br> pretrained layer by layer and then fine tuned by back propagation.\n",
    "![Screenshot%20from%202018-08-23%2019-07-35.png](Screenshot%20from%202018-08-23%2019-07-35.png)![Screenshot%20from%202018-08-23%2019-05-57.png](Screenshot%20from%202018-08-23%2019-05-57.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we prepare the data and convert it to the pytorch tensors as we did for the boltzmann machines\n",
    "\n",
    "### Now we build the architecture for the Neural network.\n",
    "1. we prepare the stacked autoencoder class. which will be inherited from nn.Module class. to get all the methods and variables from the parent class we use `super(classname,self).__init__()`<br>\n",
    "2. we make the neural net using the `Linear()` class from `nn`, then use `Sigmoid` activation function for the neural layer.<br>\n",
    "3. Lets code the forward method which will be responsible to forward the data in the network.<br>\n",
    "4. initialize the SAE().<br>\n",
    "5. lets initialize the criterion which will be the `MSEloss()`<br>\n",
    "6. RMSprop as the optimizer.for this we get all the parameters from `sae` and then initialize the parameters----> `lr` learning rate, `weight_decay` for regulation<br>\n",
    "<p>\n",
    "    \n",
    "    \n",
    "### Training the SAE:\n",
    "1. choose number of epochs<br>\n",
    "2. initialize the train loss.<br>\n",
    "3. to avoid the computation for the people who didn't rate any movie we use `s`.<br>\n",
    "4. `training_set[id_user]` is a vector, and the network can't accept single vector of one dimention, it can rather accept a batch of input vectors. To solve this we add a fake dimention which will correspond to batch. To create the new dimention, we  `Variable` function on the vector and then use method `unsqueeze` and pass the index for the fake dimention as 0.<br>\n",
    "5. We copy the input vector to create the target vector as we will work on the input vector and it will change.<br>\n",
    "6. we compute the output<br>\n",
    "7. now to avoid the computations for gradient descent using the target.<br>\n",
    "8. to avoid computations for the ones having target 0 we directly set the corresponding output to 0.<br>\n",
    "9. computing the loss error.<br>\n",
    "10. To get the users who gave the non zero ratings we take the mean corrector.<br>\n",
    "11. then we direct the loss backward<br>\n",
    "12. Then we calculate the train_loss  and increment the number of users who atleast rated one movie.<br>\n",
    "13. using optimizer, we optimize the weights. `step()` decides the intensity of the update of the weights.<br>\n",
    "14. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
